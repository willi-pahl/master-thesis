In diesem Kapitel werden die Rahmen- und Randbedienungen für das methodische Vorgehen der Evaluation großer Sprachmodelle festgehalten, welche für die Codegenerierung eingesetzt werden. Dies umfasst die Festlegung der verwendeten LLMs, die geprüfte Programmiersprachen, die Frameworks zur Erstellung und Auswertung der Tests und die Systeme welche für die Bereitstellung und Verarbeitung der Ergebnisse zum Einsatz kommen. Die Evaluierung der Modelle erfolgt in deutscher Sprache, bei der Optimierung werden einige Modelle mit englischen Proben getestet.

%---------------------------------------------------------------------------------------------------


\section{Definition der Evaluierungsziele}
% Dieser Abschnitt stellt sicher, dass die Evaluation messbar und reproduzierbar ist.
% Welche Aspekte der Codegenerierung sollen evaluiert werden? (z.B. Korrektheit des generierten Codes, Performance (Ausführungsgeschwindigkeit), Codequalität (Lesbarkeit, Wartbarkeit), Einhaltung von Coding Standards, Sicherheit, etc.)
Ausgehend, von den in Kapitel \ref{sec:goals_of_the_work} aufgestellten Thesen dieser Arbeit, werden hier die Konzepte und Designs für die Evaluation und Optimierung besprochen. Es wird dargelegt wie Untersuchen durchgeführt werden, um valide Aussagen zu den Thesen treffen zu können. Es wird darauf geachtet das die Ergebnisse nachvollziehbar und überprüfbar sind.
Die Evaluation soll zeigen, inwieweit die LLMs korrekte Ergebnisse liefern und für die Webanwendungsentwicklung geeignet sind. Um die generierten Ergebnisse Vergleichen zu können, wird hierfür ebenfalls der HumanEval-XL Benchmark eingesetzt.\vspace{0.2cm} 
%Bei der Evaluierung der optimierten Abfragen wird der Fokus zusätzlich auf die Codequalität liegen. Hierbei wird auf die Codingstandards der Programmiersprache geachtet, die Lesbarkeit und die Dokumentation innerhalb des Codes bewertet. Für die Evaluation werden wie beim HumanEval-XL Benchmark Test geschrieben und zusätzlich werden Tool zur Codebewertung eingesetzt. Trotz der Wichtigkeit der Aspekte werden bei den Tests Aspekte zur Performance und Sicherheitsaspekte vernachlässigt. Es geht in erster Linie darum zu evaluieren, ob die Brauchbarkeit und Verständlichkeit gegeben ist und sich somit die erste These beweisen lässt, das LLMs zur Steigerung der Effizienz und Verbesserung der Codequalität beitragen.\vspace{0.2cm}

%* Welche konkreten Metriken werden zur Messung dieser Aspekte verwendet? (z.B. Anzahl der Fehler im generierten Code, Ausführungszeit in Millisekunden, statische Codeanalyse-Metriken, etc.)
Für die Messung zur \glqq Wahrscheinlichkeit das eine korrekte Antwort unter den TOP Antworten ist\grqq, wird die \texttt{pass@k} Methode, für die Überprüfung der generierten Lösungen angewandt. Für jede Probe liefert der HumanEval-XL eigene Tests mit. Dieser Test zusammen mit dem generierten Code sollten einen ausführbaren und testbaren Code ergeben. Daraus kann dann mit der \texttt{pass@k} Methode die repräsentative Zuverlässigkeit des Modells für jedes Problem ermittelt und anschließend für das gesamte Modell berechnen.\vspace{0.2cm}

Die Proben aus dem HumanEval-XL Benchmark beschreiben grundlegende Verständnisfragen für die Modelle und sind mit einer einzelnen Funktion zu beantworten. Somit kann das grundlegende Verständnis einer LLM in Hinblick auf eine, für die Webanwendungsentwicklung relevante Programmiersprache evaluiert werden.\vspace{0.2cm}

Nach der Evaluierung wird versucht einige Modelle mit verschiedenen Ansätzen zu optimieren. Anschließend werden diese Ergebnisse mit den Evaluierungen ohne Optimierung vergleichen. Hier soll geprüft werden, ob sich die dritte These, aus Kapitel \ref{sec:goals_of_the_work} beweisen lässt und eine Optimierung für die Webanwendungsentwicklung herbeizuführen ist, ohne die Modelle grundlegend zu ändern.
% Oft werden mehrere Webtechnologien als Antwort erwartet, beispielsweise für Abfragen zu Webseiten könnten hier Technologien aus den Bereichen HTML, CSS, JavaScript und PHP zusammenspielen, um eine funktionsfähige Webanwendung zu generieren. Aus diesem Grund werden für die Optimierung neben dem HumanEval-XL Benchmark noch eigene komplexere Proben mit Tests erstellt. Das Design dieser Proben soll auf objektorientierter Programmierung beruhen und mehrere Probleme der Webprogrammierung abdecken. 

%---------------------------------------------------------------------------------------------------


\section{Auswahl der LLMs und deren Konfiguration}\label{subsec:llm_selection}
% Dieser Abschnitt legt die experimentelle Basis fest
% Welche LLMs werden konkret in den Experimenten verwendet? (Begründung basierend auf den Ergebnissen des Grundlagenkapitels)
Für die Evaluation werden experimentell freie und kommerzielle Modelle ausgewählt und miteinander verglichen. Hauptsächlich wurden bei den freien Modellen, jene ausgewählt welche den Fokus auf die Codegenerierung legen und mit diesem Argument beworben werden. Als Referenzen sollen die kommerziellen Modelle \textit{Gemini 1.5} und \textit{ChatGPT} dienen, welches einer stetigen Verbesserung unterliegen und eine große Nutzeranzahl aufweisen.\vspace{0.2cm}

Im Folgenden werden die ausgewählten LLMs kurz vorgestellt und warum diese gewählt wurden. Die Reihenfolge stellt an dieser Stelle keine Wertung der LLM oder über deren generierten Inhalte dar.\vspace{0.2cm}

Das \textbf{Qwen2.5-Coder}-Modell zeichnet sich durch seine spezialisierte Architektur für die Codegenerierung aus. Trainiert, um sowohl syntaktisch korrekten als auch funktional hochwertigen Code zu produzieren, integriert es fortschrittliche Mechanismen zum Kontextverständnis und semantisch sinnvolle Ausgabe. Es findet Anwendung in verschiedenen Bereichen der Softwareentwicklung, insbesondere in der Web- und Anwendungsprogrammierung. Die Qwen2.5-Coder Modellbeschreibung ist \cite{qwen-2024} und \cite{hui-2024} entnommen und wird in den Arbeiten vertieft.\vspace{0.2cm}

\textbf{Deepseek-Coder-V2} ist die zweite Generation der Deepseek-Coder-Reihe, von der gleichnamigen KI-Entwicklungsfirma DeepSeek und soll verbesserte Fähigkeiten zur Codegenerierung und -optimierung bieten. Das Modell nutzt fortschrittliche Suchalgorithmen, um präzisere und effizientere Codestücke zu erstellen. Es ist insbesondere für seine hohe Genauigkeit bei der Generierung komplexer Algorithmen und Datenstrukturen bekannt. Die Modellbeschreibung ist unter anderem aus \cite{deepseek-ai-2024} und \cite{cui-2024} entnommen. Des Weiteren wird das Modell in beiden Arbeiten mit verschiedenen Open-Source und Close-Source Modellen verglichen.\vspace{0.2cm} 

Die jüngste Innovation der chinesischen KI-Entwicklungsfirma DeepSeek ist das \textbf{DeepSeek-R1} Modell. Mit seiner offiziellen Vorstellung im Januar 2025 erregte es bedeutende Aufmerksamkeit sowohl im Bereich künstlicher Intelligenz als auch an den Finanzmärkten. Laut Unternehmensaussagen gleichwertig zu closed-source -Systemen wie ChatGPT-4 oder Gemini 2.0, demonstriert das R1-Modell eine erhebliche Leistungskraft. Im Rahmen dieser Untersuchung wurde speziell die Version deepseek-r1 analysiert, die über 32 Milliarden Parameter verfügt.\vspace{0.2cm}

Die Modelle \textbf{Llama 3.1-Claude} und \textbf{Llama 3.1} gehören mit 8 Milliarden Parametern zu den kleineren Modellen von Meta. Beide Modelle basieren auf dem LLama3.1 Modell, das Llama3.1-Claude ist aber mit anderen Systemaufforderungen erstellt wurden. Hierfür wurden die Systemaufforderungen vom Claude Sonnet 3.5 der Firma Anthropic’s verwendet, nachzulesen unter \cite{ollama_page_llama31_claude}. Ein ähnliches Modell ist auf Hugging Face veröffentlicht \cite{huggingface_page_llama31_claude}. Eine Modelcard mit weiteren Informationen zum Modell ist zu finden unter \cite{meta-llama-no-date}.\vspace{0.2cm}

Ein weiteres Modell von der KI-Entwicklungsfirma \textbf{Meta}, wurden speziell zur Codegenerierung entwickelt. Hierbei handelt es sich um das Modell \textit{Codellama}, welches mit unterschiedlicher Parameteranzahl evaluiert wird. CodeLlama basiert auf Metas Llama2 Modell wurde im Januar 2024 veröffentlicht.
% Codellama:13b
Zuerst wird das Modell \textit{Codellama:13b} evaluiert. Mit dieser Anzahl von Parametern gehört das Modell zu den kleineren Modellen. Dies wurde die Gewichte auf zwei Bit quantisiert, somit ist die Größe des Modells auf 7,4 GB reduziert wurden.
% Codellama:70b
Die zweite Modellkonfiguration ist das \textit{Codellama:70b} und wurde durch eine Quantisierung Q2\_K von 39 GB auf 25 GB verkleinert. Laut \cite{meta-2023} unterstützt das Modell unter anderem Programmiersprachen wie PHP, welche für die Erstellung für Webanwendungen relevant sind.\vspace{0.2cm}

\textbf{Mistral} ist ein modernes leistungsfähiges Sprachmodell, welches nicht speziell für die Codegenerierung und -analyse entwickelt wurde. Es verwendet fortschrittliche Transformer-Architekturen und ist für eine Vielzahl von Aufgaben einsetzbar. Darunter fallen beispielsweise natürliche Sprachverarbeitung, Textzusammenfassungen, maschinelle Übersetzung und Textklassifizierung. Dieses Modell ausgewählt, um ein Modell zu evaluieren, welches nicht speziell auf Codegenerierungsaufgaben trainiert wurde. In der Arbeit \cite{eberhardinger-2024} wurde Mistral, mit verschiedenen Modelle zur Spielecodegenerierung verglichen. Während in \cite{quan-2024} eine Evaluation für natürlichsprachlicher Erklärungen, Mistral mit anderen Modellen verglichen wurde.\vspace{0.2cm}

Das Modell \textbf{ChatGPT 4} wurde von OpenAI entwickelt und ist ein vielseitig einsetzbares Close-Source Modell. Neben allgemeinen textuellen Einsatzgebieten, lässt es sich für die Codegenerierung nutzen. Seit November 2022 ist das Modell ChatGPT 3.5 für alle kostenlos nutzbar und wird von sehr vielen Nutzer eingesetzt. Mit diesen gesammelten Daten wurde das neue Modell ChaGPT 4 weiterentwickelt und trainiert. Dadurch werden die Modelle in ihren Vorhersagen kontinuierlich besser. Eine Übersicht der Modelle von OpenAI ist unter \cite{openai_model_overview} einsehbar.\vspace{0.2cm}

Mit \textbf{Gemini 1.5} präsentiert Google ein Modell zur Verarbeitung von natürlicher Sprache und stellt es für die freie Nutzung zur Verfügung. Wie in \cite{siam-2024} beschrieben, setzen auch die Gemini Modelle, Transformer-Architektur ein, was sie dazu befähigt, komplizierte Sprachmuster zu erkennen und präzise Vorhersagen zutreffen. In \cite{siam-2024} werden die Ergebnisse von Gemini 1.5, bei der Codegenerierung mit denen von ChatGPT und Copilot vergleichen. Nach \cite{elgedawy-2024} kann sich das Gemini-Ultra-Modell beim MMLU-Benchmark sogar mit menschlichen Experten messen und bedient eine breite Palette von Anwendungsbereichen. Hier wurden die Fähigkeiten zur Codegenerierung an Sicherheitsfragen im E-Commerce Bereich getestet. Unter \cite{google_gemini_model_overview} sind weitere Informationen zu den Gemini Modelle von Google einsehbar.\vspace{0.2cm}

Neben den genannten Quellverweisen sind die Seiten der jeweiligen KI-Entwicklungsfirma, gute Quellen für weiterführende Informationen zu deren Modellen. Die Tabelle \ref{tab:selected_llms} zeigt zusammenfassend die ausgewählten Modelle.\vspace{0.2cm}

\begin{table}[!ht]
	\begin{tabular}{|l|l|l|l|c|c|c|}
		\hline
		\textbf{Modell} & \textbf{Param} & \textbf{Quantisierung} & \textbf{Größe} & \textbf{Sprache} & \textbf{offen} & \textbf{EXEC} \\
		\hline
		Qwen2.5-coder     &  32b &               q4\_K\_M &  19 GB &    DE & X & Ollama \\
		DeepSeek-coder-V2 &  16b & lite-instruct-q5\_K\_S &  11 GB &    DE & X & Ollama \\
		DeepSeek-R1       &  32b &               q4\_K\_M &  19 GB &    DE & X & Ollama \\
		Llama3.1-Claude   &   8b &                  q4\_0 & 4,7 GB &    DE & X & Ollama \\
		Llama3.1          &   8b &               q4\_K\_M & 4,7 GB & DE/EN & X & Ollama \\
		Llama3.2          &   3b &               q4\_K\_M & 2,0 GB & DE/EN & X & Ollama \\
		Llama3.3          &  70b &         instruct-q2\_K &  26 GB & DE/EN & X & Ollama \\
		Codellama         &  13b &                  q4\_0 & 7,4 GB &    DE & X & Ollama \\
		Codellama         &  70b &         instruct-q2\_K &  25 GB &    DE & X & Ollama \\
		Mistral Small     &  22b &                  q4\_0 &  12 GB &    DE & X & Ollama \\
		Gemini 1.5 Pro    & k.A. &                      - &   k.A. &    DE & - & Online \\
		Gemini 1.5 Flash  & k.A. &                      - &   k.A. &    DE & - & Online \\
		ChatGPT 4 Turbo   & k.A. &                      - &   k.A. &    DE & - & Online \\
		\hline
		\hline
	\end{tabular}
	\caption{Auswahl der LLMs für die Evaluierung}
	\label{tab:selected_llms}
\end{table}

% Welche spezifischen Parameter und Einstellungen der LLMs werden verwendet? (z.B. Temperatur, maximale Tokenanzahl, Top-p Sampling, etc.)
Die Einstellung für das Abfragen der Proben, wurden bei allen Modellen, soweit es ging identisch gewählt. Ausnahmen waren die Modelle mit 70 Milliarden Parametern. Hier entfiel die Längenbeschränkung der Token.\vspace{0.2cm}

Für die Abfragen wurde eine \textit{temperature}-Wert von $0.2$ gewählt. Ein niedriger Wert veranlasst die Modelle zu deterministischen und standardisierten Antworten. Somit wird Kreativität und Zufälligkeit im generierten Code der Modelle verhindert. Die Generierung von Programmcode soll konsistenten und präzisen Code liefern.\vspace{0.2cm}

Ein hoher \textit{top\_p} Wert verlangt von den Modellen nur Antworten zu geben, die mit hoher Wahrscheinlichkeit korrekt sind. Für die Codegenerierung sollten nur die wahrscheinlichsten und syntaktisch korrekten Token angewandt werden. Für die Abfragen wird hier ein Wert von $0.95$ angesetzt.\vspace{0.2cm}

Die maximale Anzahl an Token sollte bei der Generierung von Programmcode zwischen 200 und 1000 Token eingestellt sein, je nach Umfang der Antworten. Da hier nur die Funktionsfähigkeit geprüft wird, werden Struktur und Coding-Standards vernachlässigt, sodass \textit{max\_token} auf 600 festgelegt wird. Eine Ausnahme wird es beim \textit{Llama3.1} geben, um zu prüfen, ob eine höhere Tokenanzahl bessere Ergebnisse generiert.\vspace{0.2cm}

In der Tabelle \ref{tab:params_for_llms} sind die Werte in übersichtlicher kurzer Form noch einmal dargestellt.

\begin{table}[!ht]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Modell} & \textbf{Temp.} & \textbf{max. Token} & \textbf{Top-p} \\
		\hline
		Qwen2.5-coder      &  0.2 &       600 & 0.95 \\
		Deepseek-Coder-v2  &  0.2 &       600 & 0.95 \\
		Deepseek-R1        &  0.2 &     offen & 0.95 \\
		Llama3.1           &  0.2 &  600/1200 & 0.95 \\
		Llama3.1-Claude    &  0.2 &       600 & 0.95 \\
		Llama3.2           &  0.2 &       600 & 0.95 \\
		Llama3.3           &  0.2 &       600 & 0.95 \\
		Codellama:13b      &  0.2 &       600 & 0.95 \\
		Codellama:70b      &  0.2 &     offen & 0.95 \\
		Mistral Small      &  0.2 &       600 & 0.95 \\
		Gemini 1.5 Pro     &  0.2 &       600 & 0.95 \\
		Gemini 1.5 ChatBot & k.A. &      k.A. & k.A. \\
		ChatGPT 4 Tubro    &  0.2 &       600 & 0.95 \\
		\hline
		\hline
	\end{tabular}
	\centering
	\caption{Einstellungen der Modellparameter}
	\label{tab:params_for_llms}
\end{table}

Für die lokalen Modele kommen weitere Parameter hinzu. Unter anderem wird der Parameter \textit{do\_sample} auf $False$ gesetzt, was die Modelle veranlasst den wahrscheinlichsten folgenden Token zu wählen und ein deterministisches Verhalten fördert. Ein weiterer Parameter ist \textit{return\_full\_text} der ebenfalls auf $False$ gesetzt wird. Dadurch werden nur die neu generierten Tokens zurückgegeben, was die Relevanz der Antworten fördert.\vspace{0.2cm}

% Werden die LLMs direkt über ihre APIs angesprochen oder werden Frameworks/Bibliotheken verwendet?
Alle Skripte zur Erstellung und Auswertung der Antworten werden mit Python realisiert und nutzen die jeweiligen APIs der Anbieter. Dies gilt für die offenen lokalen wie auch für die kommerziellen Modelle. Die Umsetzung erfolgt für die lokalen Modelle mit Python \texttt{langchain} und für die kommerziellen Modelle der Gemini-Reihe wird die Google eigene Bibliothek, \texttt{google.genai} verwendet. Dasselbe trifft für die Modelle von OpenAI zu, hierbei kommt die Python Bibliothek \texttt{openai} zum Einsatz.

%---------------------------------------------------------------------------------------------------


\section{Design der Evaluierung}
% Dieser Abschnitt ist zentral für die Arbeit, da er die methodische Vorgehensweise der Evaluation beschreibt.
In dem Kapitel wird das Design der Evaluierung besprochen. Der verwendete Benchmark wird vorgestellt und anschließend erläutert wie die Durchführung der Evaluierung erfolgt. Als letzter Punkt wird in diesem Kapitel die Art und Weise festgehalten, wie die Daten dokumentiert werden, um die Überprüf- und Nachvollziehbarkeit zu gewährleisten.


\subsection{HumanEval-XL Benchmark}\label{subsec:structor_of_humaneval_xl}
Das Experiment wird mit dem HumanEval-XL Benchmark durchgeführt. Dieser Benchmark besteht aus einer Reihe von 80 Proben in verschiedenen Programmier- und Landessprachen, die wie folgt aufgebaut sind,

\begin{myenumerate}
	\item \textbf{task\_id}: Die Kennung der Datenprobe, als eine eindeutige ID. An ihr ist bereits die Programmiersprache erkennbar, welche verwendet wird. Ein Beispiel für die PHP-Proben ist \texttt{php/0} oder \texttt{php/1}, für JavaScript wäre das beispielsweise \texttt{javascript/0}.
	\item \textbf{prompt}: Der Prompt ist die Anfrage für das Modell und besteht aus Funktionsheader und Docstring. Hier ist die eigentliche Aufgabe der Probe definiert. Dieser Teil des Benchmarks wird später in diesem Kapitel, für die PHP Proben als \textit{Opening Tag} und \textit{Kommentar} ausführlicher beschrieben.
	\item \textbf{entry\_point}: Der Einstiegspunkt für die Probe ist der \texttt{entry\_point}, in welcher der zu verwendende Methodenname explizit genannt wird. Dieser muss im Ergebnis und Test angewandt werden.
	\item \textbf{test}: Die vordefinierte Testaufgaben, für die geforderte Funktion im generierten Code, wird ebenfalls später in diesem Kapitel ausführlicher beschrieben.
	\item \textbf{description}: Diese enthält ausführliche Beschreibung der Aufgabe des Benchmarks, welche für den Nutzer bestimmt ist und nicht für das Modell.
	\item \textbf{language}: Mit Kennung der Programmiersprache wird nochmals explizit auf diese hingewiesen, für die das Modell eine Lösung generieren soll.
	\item \textbf{canonical\_solution}: Diese Lösung für das Problem findet keine Verwendung, da sie im HumanEval-XL nicht verwendet wird und zudem keine Angaben enthält.
	\item \textbf{natural\_language}: Hier findet sich die Angabe zur Ländersprache, in der die Proben erstellt wird.
\end{myenumerate}

Die Abbildung \ref{img:code_generation_humaneval} zeigt den Aufbau und damit alle wichtigen Bereiche des Benchmarks. In dessen Struktur sind vier wesentliche Teilbereiche erkennbar. Dazu gehören das \textit{Opening Tag} (optional), der \textit{Kommentar}, das \textit{Ergebnis} und der \textit{Test}.\vspace{0.2cm}

\begin{figure}[!ht]
	\includegraphics[width=0.7\textwidth]{content/chapter_concept_design/images/code_generation_humaneval_x.eps}
	\centering
	\caption{Aufbau des HumanEval-XL Benchmarks}
	\label{img:code_generation_humaneval}
\end{figure}

Das \textit{PHP Opening Tag}, in der Abbildung \ref{img:code_generation_humaneval} mit \textbf{1} bezeichnet, ist eine optionale Angabe zur Programmiersprache und durch den Benchmark vorgegeben. Während bei den PHP-Proben das Opening Tag \texttt{<?php} lautet, ist beispielsweise bei den JavaScript-Proben kein Opening Tag angegeben. Hier beginnt die Probe direkt mit dem Kommentar. Der in der Abbildung \ref{img:code_generation_humaneval} mit \textbf{2} bezeichnete Kommentar, enthält die eigentliche Aufgabe, die das Modell lösen soll. Auch der Kommentar ist vom Benchmark vorgegeben. Die letzte Zeile enthält den Methodennamen als Codevorgabe, der zwingend verwendet werden muss. Der Methodenname ist erforderlich, um nach dem Generieren der Antwort einen Test durchführen zu können. Die Listings \ref{lst:example_probe_1_of_benchmark} zeigt ein Beispiel für die beiden Bereiche der ersten Probe des Benchmarks. In dem Listing ist auch zu erkennen das es sich bei den Proben, um Few-Shot-Prompting handelt. Die Aufgaben werden mit mehreren Beispiellösungen angegeben.\vspace{0.2cm}

\begin{figure}[!ht]
\begin{lstlisting}[
	language=php,
	label=lst:example_probe_1_of_benchmark,
	caption={PHP Beispiele für die erste Probe}
]
<?php

/**
* Sie sind ein erfahrener PHP-Programmierer und hier ist Ihre Aufgabe.
* Sie erhalten eine Liste von Einzahlungs- und Abhebungsvorgängen auf einem Bankkonto, das mit einem Nullsaldo beginnt. Ihre Aufgabe besteht darin, festzustellen, ob zu irgendeinem Zeitpunkt das Guthaben des Kontos unter Null fällt, und an diesem Punkt sollte die Funktion True zurückgeben. Andernfalls sollte sie False zurückgeben.
* >>> below_zero([1, 2, 3])
* False
* >>> below_zero([1, 2, -4, 5])
* True
*
*/
function belowZero($operations){
\end{lstlisting}
\end{figure}
	
Die beiden Teilbereiche \textit{Opening Tag} und \textit{Kommentar} werden an das Modell als Eingabeaufforderung übergeben. Es generiert den Programmcode und liefert diesen als \textit{Ergebnis} zurück. Die Modelle sollten im Idealfall den vorgegebenen Methodennamen aufgreifen und diesen in ihrer Ausgabe verwendet. Somit findet sich der Methodenname auch im Teilbereich \textbf{3} wieder. Ebenfalls werden oft das Opening Tag und der Kommentar durch das Modell übernommen. In der Abbildung \ref{img:code_generation_humaneval} ist das Ergebnis mit \textbf{3} ausgezeichnet. Der Bereich \textbf{4} bildet den letzten Teil des Benchmarks. Der Test ist ebenfalls vom HumanEval-XL Benchmark vorgegeben. Hier wird ein einfacher Vergleich durchgeführt der bei Abweichung von der Vorgabe eine Exception auslöst. Dazu werden an die vordefinierte Methode Parameter übergeben, aus denen der generierte Code eine Lösung errechnet. Diese wird mit einer, durch den Benchmark vordefinierten Lösung abgeglichen. Das Listing \ref{lst:example_test_1_of_benchmark} zeigt den Aufbau des Tests, am Beispiel der ersten Probe des Benchmarks. Hier sind in Zeile fünf und elf die erwarteten Methodennamen und die zu übergebenen Parameter zusehen. Ebenfalls sind die Vergleichswerte in Zeile vier und zehn angegeben und die einfache Methode \texttt{compare} in Zeile eins bis drei, welche einen statischen Vergleich durchführt. Der hier gezeigte Test der ersten Probe ist in einer gekürzten Fassung dargestellt. Der vollständige Code zur Probe kann unter \href{https://github.com/willi-pahl/master-thesis/blob/main/data/problems/PHP_German.jsonl}{https://github.com/willi-pahl/master-thesis/blob/main/data/problems/PHP\_German.jsonl} entnommen werden.\vspace{0.2cm}

\begin{figure}
\begin{lstlisting}[
	language=php,
	label=lst:example_test_1_of_benchmark,
	caption={PHP Beispiele für den Test der ersten Probe}
]
function compare($x, $y) {
	return $x == $y;
}
$arg00 = [];
$x0 = belowZero($arg00);
$v0 = false;
if (!compare($x0, $v0)) {
	throw new Exception("Error at 1th assert statement.");
}
$arg10 = [1, 2, -3, 1, 2, -3];
$x1 = belowZero($arg10);
$v1 = false;
if (!compare($x1, $v1)) {
	throw new Exception("Error at 2th assert statement.");
}
// more tests
\end{lstlisting}
\end{figure}


\subsection{Durchführung der Evaluierung}
Nach der Generierung des Codes durch das Modell werden die Bereiche \textbf{3} (Ergebnis) und \textbf{4} (Test) für die Evaluierung der Probe verwendet. Hierfür werden die Bereiche zusammengeführt und ergeben einen ausführbaren und testbaren Code. Die Ergebnisse werden dokumentiert und dienen für die Berechnung der Wahrscheinlichkeit, mit \texttt{pass@k}, dass ein Modell unter $k$-Proben eine korrekte Antwort liefert. Der Ablauf der Evaluation ist in Abbildung \ref{img:sequence_of_evaluation} dargestellt.\vspace{0.2cm}

\begin{figure}[!ht]
	\includegraphics[width=\textwidth]{content/chapter_concept_design/images/ablauf_evaluation.eps}
	\centering
	\caption{Ablauf der Evaluierung mit dem HumanEval-XL Benchmarks}
	\label{img:sequence_of_evaluation}
\end{figure}

%*   Welche Arten von Code sollen generiert werden? (z.B. einfache HTML-Formulare, komplexe JavaScript-Funktionen, serverseitiger Code in Python/Node.js, etc.)
Um die Modelle zu evaluieren und ihre Fähigkeiten hinsichtlich Code für Webanwendungsentwicklung zu generieren, werden die Tests in den Programmiersprache PHP und in der deutschen Version vorgenommen. Dafür sollen die Modelle mehrfach einfache Funktionen generieren. Alle Modelle werden im ersten Schritt mit denselben Parametern getestet, die bereits in der Tabelle \ref{tab:params_for_llms} dokumentiert wurden.\vspace{0.2cm}

%*   Wie werden die Testfälle für die Evaluation generiert? (z.B. manuelle Erstellung, automatische Generierung, Verwendung von bestehenden Code-Snippets, etc.) Wie groß ist der Umfang der Testfälle? (Anzahl der zu generierenden Code-Snippets)
Der Benchmark liefert für jede Probe einen Test für die Evaluierung mit. Dafür werden in den Prompts die Namen der Methoden und die zu übergebenen Parameter angegeben, welche durch die Modelle zu erstellen sind. Der jeweilige Test verwendet dann diesen Namen und übergibt die geforderten Parameter. Von jeder Probe werden pro Modelle fünf Varianten erstellt. Die Ergebnisse werden in einer Liste chronologisch gespeichert. Mithilfe der pass@k Methode erfolgt die Evaluierung der Ergebnisse und anschließend die Bewertung der Modelle.


\subsection{Dokumentation der Daten}
Die verwendeten Dateien des HumanEval-XL Benchmarks sind im \textit{JSONL}-Format vorgegeben. Ebenso werden die von der LLM generierten Codes in diesem Format dokumentiert und persistent gespeichert. Für jede Probe wird eine separate Datei erstellt, die jeweils fünf Antworten von einer LLM enthält. Für den gesamten Benchmark sind, nach dem Test pro LLM, 80 \textit{JSONL}-Dateien vorhanden.\vspace{0.2cm}

Neben den Rohdaten werden die Ergebnisse in verschiedenen \textit{Open Document Speadsheet} (ODS) Dokumente übernommen und aufbereitet. Die Dateien lassen sich mit Tabellenkalkulationstools von OpenOffice oder LibreOffice öffnen. Diese dienen für die Zusammenfassungen der Ergebnisse und zusätzlich werden hieraus Grafiken generiert, welche auch in diesem Dokument Verwendung finden.\vspace{0.2cm}

Als Letztes werden die programmierten Klassen und Methoden in Python-Dateien (\texttt{*.py}) abgelegt, mit denen die Ergebnisse erstellt und evaluiert werden. Somit lassen sich alle erstellten Ergebnisse nachvollziehen.

%---------------------------------------------------------------------------------------------------


\section{Konzeption der Optimierung}\label{sec:conzept_of_optimization_prompt}
%*   Welche Strategien für das Prompt-Engineering werden untersucht? (z.B. Few-Shot-Prompting, Chain-of-Thought-Prompting, Verwendung von Code-Kommentaren als Prompts, etc.)
%Während die Evaluierung der Modelle ausschließlich mit den Proben des HumanEval-XL erfolgten, wird die Optimierung der Prompts, an zusätzlichen eigenen erstellten Proben erfolgen. Diese sind komplexer als die allgemeinen Proben aus dem HumanEval-XL Benchmark. Hierbei soll untersucht werden, mit welchem Ansätzen Prompts im Bereich der Codegenerierung für die Webprogrammierung erfolgen kann. Es wird neben der Proben auch ein Unittest vorgegeben. Somit kann die Codeüberprüfung der generierten Snippets mit Unittests erfolgen. Der Vorteil von Unittest ist, dass alle Tests durchgeführt werden, auch wenn ein vorheriger Test fehlschlägt. Um diese Tests ausführen zu können, sind PHP Dateien erforderlich. Somit müssen die generierten Codes in Dateien gespeichert und können im Anschluss geprüft werden.\vspace{0.2cm}
%Für die Evaluierung der Ergebnisse der Optimierung wird manuell und automatisiert erfolgen. Durch die automatisierte Evaluierung ist auch hier der Vorteil gegeben das die Ergebnisse der ausgewählten Modelle nach dem gleichen Verfahren beurteilt werden. Wie bei der Evaluierung der Modelle werden die Ergebnisse auch hier festgehalten und dokumentiert.\vspace{0.2cm}
%In der Arbeit werden verschiedene Möglichkeiten der Optimierung untersucht, die in den folgenden Kapiteln vorgestellt und deren Durchführung erläutert werden.
Die Prompts aus dem HunamEval-XL Benchmark sind, wie bereits erwähnt als  \textit{Few-Shot-Prompts} verfasst. Einige der Proben besitzen nur ein Beispiel und sind somit als \textit{One-Shot-Prompts} definiert, was in Listing \ref{lst:example_test_3_of_benchmark} beispielhaft dargestellt wird. Hier ist die dritte Probe des Benchmarks zu sehen. Vor den Beispielen ist die eigentliche Aufgabe, welche von der LLM gelöst werden muss. Diese ist in Zeile drei bis sechs zu sehen. In der Zeile sieben ist der beispielhafte Methodenaufruf für die LLM dargestellt und das erwartete Ergebnis folgt in Zeile acht. Die Zeile elf zeigt in diesem Beispiel den vordefinierten Methodennamen und die zu übergebenen Parameter für die Methode.\vspace{0.2cm}

\begin{figure}[!ht]
\begin{lstlisting}[
	language=php,
	label=lst:example_test_3_of_benchmark,
	caption={PHP Beispiele für den Test der dritten Probe, One-Shot-Prompt}
]
<?php

/**
 * Sie sind ein erfahrener PHP-Programmierer und hier ist Ihre Aufgabe.
 * Die Eingabe sind zwei Zeichenketten a und b, die nur aus 1en und 0en bestehen.
 * Führen Sie eine binäre XOR-Operation auf diesen Eingaben aus und geben Sie das Ergebnis ebenfalls als Zeichenkette zurück.
 * >>> string_xor('010', '110')
 * '100'
 *
 */
function stringXor($a, $b){
\end{lstlisting}
\end{figure}

Anders als bei \textit{Zero-Shot-Prompts}, können LLMS durch die Angabe von Beispielen, die Regeln besser erlernen, Muster und Konzept der Aufgabe ableiten, helfen der LLM beim Erkennen des erwarteten Formates und reduzieren Halluzinationen bei den Antworten. Somit wird eine Verbesserung des generieren Programmcodes erreicht. Durch das Anwenden dieses Prompt-Design wurden die Eingabeaufforderungen bereits durch die Autoren des HumanEval-XL optimiert bereitgestellt.\vspace{0.2cm}

Neben der Optimierung des Prompt Designs wurden weitere Optimierungen in den Eingabeaufforderungen des HumanEval-XL angewandt. So werden die Eingabeaufforderung bereits als Kommentare der jeweiligen Programmiersprache verfasst. Bei den Proben für die Programmiersprache PHP wird vor dem Kommentar, dass für PHP Programme erforderliche \textit{Opening Tag} gesetzt. Somit wird ein weiterer Kontextparameter definiert und liefert für die LLM einen weiteren Anhaltspunkt für die zu generierende Programmiersprache.\vspace{0.2cm}

Eine weitere Optimierung der Eingabeaufforderung ist die bereits angesprochene letzte Zeile. Hier wird der Name für die erwartete Funktion angegeben und es wird die Fähigkeit der Modelle ausgenutzt, Funktionen und Codes zu vervollständigen. Dies ist wichtig, da die Tests im Benchmark auf einen existierenden Funktionsnamen angewiesen sind.\vspace{0.2cm}

Somit lässt sich feststellen, dass die Eingabeaufforderungen weitestgehend optimiert sind. Daher wird auf eine weitere manuelle Optimierung der Eingabeaufforderung verzichtet. Es gibt viele Arbeiten, die sich mit genau diesem Problem befassen und es sind durchaus weitere Möglichkeiten vorhanden, die Eingabeaufforderungen textuell zu optimieren.\vspace{0.2cm}


\subsection{Optimierung durch Frameworkauswahl}
Eine andere Methode zur Optimierung der Eingabeaufforderungen ist die Verwendung verschiedener Frameworks. Hierbei werden die Eingabeaufforderungen durch das Framework angepasst, beispielsweise durch das Hinzufügen zusätzlichen Meta-Angaben. Es soll untersucht werden wie sich die Verwendung unterschiedlicher Frameworks auf die Ausgaben der Modelle auswirkt. In der Arbeit werden die Auswirkungen auf die Ergebnisse, der Python-Frameworks \textit{langchain} und \textit{DSPy}, für verschiedene Modelle verglichen. Für eine automatisierte Auswertung der Tests wird hier ebenfalls der HumanEval-XL angewandt.\vspace{0.2cm}

Das Framework \textit{langchain} wurde 2022 vorgestellt und ermöglicht Nutzern die Entwicklung komplexer Anwendungen für Interaktionen mit LLMs. Jedoch ist Fachwissen im Bereich Prompt-Engineering erforderlich, um optimale Eingabeaufforderungen an die Modelle zu stellen. Dieses Fachwissen ist beim \textit{DSPy}-Framework nicht mehr notwendig. Hier übernimmt das Framework die Optimierung der Eingabeaufforderungen für die Modelle und macht somit das manuelle Optimieren der Prompts und Kenntnisse in diesen Techniken überflüssig. \textit{DSPy} ist im Oktober von den Forschern der \textit{Stanford NLP Group} veröffentlicht worden, die an Standards für \textit{Natural Language Processing} arbeiten.\vspace{0.2cm}


\subsection{Evaluierung der Optimierungen}
% Werden verschiedene Prompt-Varianten für die gleichen Code-Generierungsaufgaben verwendet, um deren Einfluss auf die Ergebnisse zu untersuchen?
% Dieser Abschnitt ist besonders wichtig, da er sich mit der Optimierung der LLMs durch Prompt-Engineering beschäftigt.
Wie auch schon bei der Evaluierung der Modelle, werden die Ergebnisse der Optimierung ebenfalls festgehalten, um diese nachvollziehen zu können. Die erhobenen Daten werden in Dateien, die im \textit{JSONL}-Format vorliegen gespeichert. Für die Auswertung der Ergebnisse ist auch hier das \textit{Open Document Speadsheet (ODS)} vorgesehen. Hier erfolgt die Zusammenfassung der Ergebnisse und deren grafischen Darstellung.\vspace{0.2cm}

Die erstellen Methoden und Klasse, für die Erzeugung der Antworten und Ausführen der Evaluierungen, sind in Python erstellt und in Dateien \texttt{*.py} abgelegt, um die Nachvollziehbarkeit der Ergebnisse zu gewährleisten.

%---------------------------------------------------------------------------------------------------


\section{Testumgebung}
%*   Welche Hardware und Software werden für die Experimente verwendet? (z.B. CPU, GPU, Betriebssystem, Programmiersprachen, Bibliotheken, etc.)
Die \textit{Open-Source}-Modelle laufen auf einem System, welches mit 8 Kernen (16 Threads) und 32 GB \acrshort{RAM} ausgestattet ist. Um zusätzlichen Speicher zu erhalten, wurde eine 100 GB Swap Partition genutzt. Zur weiteren Optimierung wurde eine RTX 3060 Grafikkarte der Firma Nvidia, mit 12 GB \acrshort{VRAM} verwendet. Das System ist mit dem lokalen Netzwerk verbunden und stellt den Zugriff der Modelle über das Netzwerk bereit.\vspace{0.2cm}

Auf dem System ist ein Debian 12 installiert und für die Bereitstellung kommt das freie Framework Ollama zum Einsatz. Zur Evaluierung wurde PHP in der Version 8.2.26 (Build-Stand: 25.11.2024) angewandt.\vspace{0.2cm}

Über die Hard- und Softwarekomponenten der kommerziellen \textit{Cloused-Source}-Modelle sind keine Hardwarespezifikationen oder andere tiefer gehende Informationen vorhanden. Da die Performance für die Evaluierung in dieser Arbeit keine entscheidende Rolle spielt, wird nicht weiter recherchiert.\vspace{0.2cm}

%*   Wie wird die Reproduzierbarkeit der Experimente sichergestellt? (z.B. Verwendung von Versionskontrolle, Dokumentation der Umgebung, etc.)
Alle wichtigen Codes und Codesequenzen, sowie die Ergebnisse sind im Dokument oder im Anhang zu finden. Der gesamte Code und die Ergebnisse sind dokumentiert, sodass die Möglichkeit besteht die Evaluierungen nachzuvollziehen. Alle Daten und Dateien werden unter \href{https://github.com/willi-pahl/master-thesis}{https://github.com/willi-pahl/master-thesis} bereitgestellt.\vspace{0.2cm}
