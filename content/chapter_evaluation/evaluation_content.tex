Die Evaluation der Ergebnisse erfolgt im ersten Schritt anhand des HumanEval-XL Benchmarks. Dieser Benchmark wird in \cite{peng-2024} vorgestellt und erweitert den HumanEval \cite{chen-2021}. Der HumanEval-Benchmark evaluiert nur Python während der HumanEval-XL weitere Programmiersprachen unterstützt und in verschiedenen Sprachen erhältlich ist, darunter auch die deutsche Sprache. Neben Python werden auch PHP und JavaScript, welche für die Webentwicklung wichtig sind. Die Datensätze des HumanEval-XL sind unter \href{https://github.com/FloatAI/humaneval-xl}{https://github.com/FloatAI/humaneval-xl} einsehbar und bestehen jeweils aus 80 Tests.\vspace{0.2cm}

Diese Tests fordern LLM's auf kleine Problem zu lösen. Aus diesem Grund werden weitere Tests mit größeren Problemblöcken erstellt und evaluiert. Die Evaluierung erfolgt zunächst manuell und anhand folgend genannter Kriterien.\vspace{0.2cm}

Ein Kriterium ist die Funktionalität des generierten Codes. Hierbei wird geprüft, ob alle geforderten Kriterien von der LLM umgesetzt wurden.\vspace{0.2cm}

Des Weiteren ist eine Syntax und Fehlerfreiheit Evaluation vorgesehen. Hier werden die Regeln und Coding-Standards der jeweiligen Programmiersprache und die Fehlerfreiheit während der Laufzeit geprüft. Für die Prüfung der Coding-Standards wird ein SonarQube-Server verwendet. Optional werden einige Tests von zusätzlichen Tools validiert, beispielsweise bei der Validierung von PHP Files sind es Tools wie phpunit\footnote{phpunit kann mit \textit{composer} installiert werden und steht unter \href{https://github.com/sebastianbergmann/phpunit}{https://github.com/sebastianbergmann/phpunit} zum Download bereit.} und Code\_Sniffer\footnote{Code\_Sniffer steht unter \href{https://github.com/squizlabs/PHP_CodeSniffer}{https://github.com/squizlabs/PHP\_CodeSniffer} zum Download zur Verfügung und wird ebenfalls mittels \textit{composer} installiert.}. Für die Validierung von JavaScript findet das Framework Jasmin\footnote{\href{https://jasmine.github.io/}{https://jasmine.github.io}.} Anwendung.


Ebenfalls wird die Qualität des Codes evaluiert. Das Augenmerk liegt auf die Lesbarkeit, Effizienz und Wartbarkeit des generierten Codes.\vspace{0.2cm}

Ein letzter Punkt der evaluiert wird, ist die Performance.


%Bewertet wird die Korrektheit des generierten Codes. Hierbei liegt das Augenmerk auf die Syntax, Semantik und die Fehlerfreiheit.\vspace{0.2cm}

%Eine weitere Rolle spielt die Codequalität, hier werden Lesbarkeit, Wartbarkeit und Best Practices bewertet.\vspace{0.2cm}


\section{Bewertung der Modelle}
Für die Bewertung wird das Vorgehen gewählt, welche in \cite{chen-2021} und \cite{peng-2024} beschrieben sind. Diese werden für die Webentwicklung relevanten Sprachen PHP und JavaScript exemplarisch erweitert. Die Evaluierung der Modelle wird auf den Ebenen \glqq einfache Fragen\grqq , \glqq mittelschwere Fragen\grqq \ und \glqq komplexe Aufgaben\grqq \ erfolgen. Die \glqq einfachen Fragen\grqq \ werden bereits durch die zuvor genannten Benchmarks abgedeckt, sodass der entwickelte Fragenkatalog sich auf beiden Ebenen \glqq mittelschweren Fragen\grqq \ und \glqq komplexen Aufgaben\grqq \ konzentriert.\vspace{0.2cm}

% https://ki-techlab.de/ki-news/evaluierung-grosser-sprachmodelle-ein-technischer-leitfaden/

\begin{tcolorbox}[
	enhanced,
	colback=BhtColorYellow!5!white,
	colframe=BhtColorYellow!75!black,
	title= HTML Startseite
	]
	Schreibe eine einfache HTML-Seite für die Startseite eines Blogs.
\end{tcolorbox}

\begin{tcolorbox}[
	enhanced,
	colback=BhtGrey!5!white,
	colframe=BhtGrey!75!black!50,
	title= ChatGPT 3.5
	]
	\begin{verbatim}
		<!DOCTYPE html>
		<html lang="de">
		<head>
		  <meta charset="UTF-8">
		  <meta name="viewport" content="width=device-width, initial-scale=1.0">
		</head>
		<body>
		  <div class="container"></div>
		</body>
		</html>
	\end{verbatim}
\end{tcolorbox}
