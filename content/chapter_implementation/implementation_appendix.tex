\section{Installationshinweise}
\subsection{Python}
Da in dieser Arbeit Python verwendet wird, sollte zu den grundlegenden Paketen, für die Arbeit mit großen Sprachmodellen folgende Zusatzpakete installiert sein.

\begin{verbatim}
	pip3 install langchain
	pip3 install ollama
	pip3 install transformer
\end{verbatim}

Im Weiteren wird kein Hinweis auf verwendete Pakete gegeben. Diese sind evtl. den Fehlermeldungen während und nach der Programmausführung zu entnehmen.

\subsection{Installation und Konfiguration von Ollama}\label{sec:install_config_ollama_local}
Für die Installation von Ollama wird bei Linux folgendes Skript ausgeführt,

\begin{verbatim}
	curl -fsSL https://ollama.com/install.sh | sh
\end{verbatim}

Ollama kann in seiner Konfiguration angepasst werden, im Folgenden wurde der Pfad zur den Modellen geändert und die Erreichbarkeit von Ollama über Netzwerk eingestellt. Dazu wird die Datei  \texttt{/etc/systemd/system/ollama.service} angepasst und der korrekte Host und IP-Adresse gesetzt.

\begin{lstlisting}[language=diff,caption={Ollama Hostanpasssng für Netzwerkbetrieb}]
	diff --git a/ollama.service b/ollama.service
	--- a/ollama.service
	+++ b/ollama.service
	@@ -10,3 +10,4 @@
	RestartSec=3
	Environment="PATH=/usr/local/bin:/usr/bin"
	-   
	+   Environment="OLLAMA_HOST=0.0.0.0"
	+   Environment="OLLAMA_MODELS=/home/ai/models"
	+   
\end{lstlisting}

Nach der Installation kann die Funktionsfähigkeit geprüft werden, der folgenden Beispielaufruf lädt ein Modell von Ollama und startet dieses.

\begin{verbatim}
	ollama run deepseek-coder-v2:16b
\end{verbatim}

Im Anschluss kann über die Konsole mit dem Modell interagiert werden.

%---------------------------------------------------------------------------------------------------


\subsection{Open WebUI Installationshinweise}\label{sec:open_webui}
Hier wird Open WebUI als docker Container verwendet, es ist also erforderlich vorher docker zu installieren. Die Installation von Open WebUI, unter Debian kann mit folgendem Skript erfolgen. 

%\begin{verbatim}
\begin{lstlisting}[language=bash,caption={Open WebUI installieren}]
# Pull Open WebUI container.
docker pull ghcr.io/open-webui/open-webui:main

# Run container.
docker run -d --network=host -v open-webui:/app/backend/data \
-e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui \
--restart always ghcr.io/open-webui/open-webui:main
\end{lstlisting}
%\end{verbatim}

Der Aufruf der UI, kann mittel Browser erfolgen. Hier wird die IP und der Port 8080 angegeben. Beispiel \texttt{http://192.168.2.45:8080}.

%---------------------------------------------------------------------------------------------------


\subsection{Download der Hugging Face Modelle}\label{sec:hugging_face_models}
Ein Hugging Face Modell kann, heruntergeladen werden. Soll das Modell auch nach dem Löschen des lokalen \texttt{huggingface}-Caches zur Verfügung stehen, sollte das Modell separat abgespeichert werden. Für den Download und speichern der Modelle kommt das Python-Skript, wie in Listing \ref{lst:download_hugging_face_model} gezeigt zur Anwendung. Hier ist zu erwähnen, dass ausreichend RAM zur Verfügung stehen muss, um die Modelle zu speichern. Mit diesem Script kann ein Modell an einem angegebenen Pfad abgespeichert werden und ist nach dem Cache löschen immer noch lokal vorhanden.

\lstinputlisting[
	language=python,
	caption={Laden der Modelle von Hugging Face und lokal speichern},
	label=lst:download_hugging_face_model
]{codes/src/download/huggingface_model.py}


\subsection{Abfragen lokaler Modelle}
Um die Modelle abzufragen, kommt der Code, welcher in Listing \ref{lst:call_ollama_model} zu sehen ist, zum Einsatz.

\lstinputlisting[
	language=python,
	caption={Abfragen der Ollama Modelle},
	label=lst:call_ollama_model
]{codes/src/execute/execute_ollama.py}


\subsection{Abfrage cloudbasierter Modelle}

\lstinputlisting[
	language=python,
	caption={Ausführen der Prompts für Gemini Modelle.},
	label=lst:execution_gemini_models
]{codes/src/execute/execute_google_gemini.py}

\lstinputlisting[
	language=python,
	caption={Ausführen der Prompts für OpenAI Modelle.},
	label=lst:execution_openai_models
]{codes/src/execute/execute_openai_gpt.py}


\subsection{Evaluation der Antworten von den Modellen}
Das Listing \ref{lst:evaluation_search_generated_code} zeigt beispielhaft eine Suche nach Codezeilen in einer Antwort eines Modells. Die Methode \texttt{search\_generated\_code} muss auf jedes Modell angepasst sein.

\begin{lstlisting}[
	language=python,
	caption={Evaluation der Modellantworten: Suche nach Codeausschnitten},
	label=lst:evaluation_search_generated_code
	]
def search_generated_code(content: str) -> str:
    """
    Search the generated code in string, it is one line of file.
    :param content (str): Answer from LLM.
    :returns str: Generated code.
    """
    for index in [i for i in range(0, 10)]:
        if content.startswith('{"result_' + str(index) + '":"'):
            generated_code: str = content.split(
                '{"result_' + str(index) + '":"')[1]
            generated_code = generated_code[:-2]
            generated_code = generated_code.strip()

        if len(generated_code.split("```php")) > 1:
            generated_code = generated_code.split("```php")[1]
            generated_code = generated_code.split("```")[0]

        if generated_code.startswith("<?php"):
            generated_code = generated_code.split("<?php")[1]
            generated_code = generated_code.split("?>")[0]

        if len(generated_code.split(r"\n}\n")) > 0:
            generated_code = generated_code.split(r"\n}\n")[0] + "\n}\n"

        return generated_code
    return ""
\end{lstlisting}
