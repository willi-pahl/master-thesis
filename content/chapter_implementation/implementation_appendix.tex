\section{Installationshinweise}
\subsection{Python}
Da in dieser Arbeit Python verwendet wird, sollte zu den grundlegenden Paketen, für die Arbeit mit großen Sprachmodellen folgende Zusatzpakete installiert sein.

\begin{verbatim}
	pip3 install langchain
	pip3 install ollama
	pip3 install transformer
\end{verbatim}

Im Weiteren wird kein Hinweis auf verwendete Pakete gegeben. Diese sind evtl. den Fehlermeldungen während und nach der Programmausführung zu entnehmen.

\subsection{Installation und Konfiguration von Ollama}\label{sec:install_config_ollama_local}
Für die Installation von Ollama wird bei Linux folgendes Skript ausgeführt,

\begin{verbatim}
	curl -fsSL https://ollama.com/install.sh | sh
\end{verbatim}

Ollama kann in seiner Konfiguration angepasst werden, im Folgenden wurde der Pfad zur den Modellen geändert und die Erreichbarkeit von Ollama über Netzwerk eingestellt. Dazu wird die Datei  \texttt{/etc/systemd/system/ollama.service} angepasst und der korrekte Host und IP-Adresse gesetzt.

\begin{lstlisting}[language=diff,caption={Ollama Hostanpasssng für Netzwerkbetrieb}]
	diff --git a/ollama.service b/ollama.service
	--- a/ollama.service
	+++ b/ollama.service
	@@ -10,3 +10,4 @@
	RestartSec=3
	Environment="PATH=/usr/local/bin:/usr/bin"
	-   
	+   Environment="OLLAMA_HOST=0.0.0.0"
	+   Environment="OLLAMA_MODELS=/home/ai/models"
	+   
\end{lstlisting}

Nach der Installation kann die Funktionsfähigkeit geprüft werden, der folgenden Beispielaufruf lädt ein Modell von Ollama und startet dieses.

\begin{verbatim}
	ollama run deepseek-coder-v2:16b
\end{verbatim}

Im Anschluss kann über die Konsole mit dem Modell interagiert werden.

%---------------------------------------------------------------------------------------------------


\subsection{Open WebUI Installationshinweise}\label{sec:open_webui}
Hier wird Open WebUI als docker Container verwendet, es ist also erforderlich vorher docker zu installieren. Die Installation von Open WebUI, unter Debian kann mit folgendem Skript erfolgen. 

%\begin{verbatim}
\begin{lstlisting}[language=bash,caption={Open WebUI installieren}]
# Pull Open WebUI container.
docker pull ghcr.io/open-webui/open-webui:main

# Run container.
docker run -d --network=host -v open-webui:/app/backend/data \
-e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui \
--restart always ghcr.io/open-webui/open-webui:main
\end{lstlisting}
%\end{verbatim}

Der Aufruf der UI, kann mittel Browser erfolgen. Hier wird die IP und der Port 8080 angegeben. Beispiel \texttt{http://192.168.2.45:8080}.

%---------------------------------------------------------------------------------------------------

\newpage
\section{Download der Hugging Face Modelle}\label{sec:hugging_face_models}
Ein Hugging Face Modell kann, heruntergeladen werden. Soll das Modell auch nach dem Löschen des lokalen \texttt{huggingface}-Caches zur Verfügung stehen, sollte das Modell separat abgespeichert werden. Für den Download und speichern der Modelle kommt das Python-Skript, wie in Listing \ref{lst:download_hugging_face_model} gezeigt zur Anwendung. Hier ist zu erwähnen, dass ausreichend RAM zur Verfügung stehen muss, um die Modelle zu speichern. Mit diesem Script kann ein Modell an einem angegebenen Pfad abgespeichert werden und ist nach dem Cache löschen immer noch lokal vorhanden.

\lstinputlisting[
	language=python,
	caption={Laden der Modelle von Hugging Face und lokal speichern},
	label=lst:download_hugging_face_model
]{codes/src/download/huggingface_model.py}

\newpage
\section{Abfragen lokaler Modelle}
Um die Modelle abzufragen, kommt der Code, welcher in Listing \ref{lst:call_ollama_model} zu sehen ist, zum Einsatz. Hier wird die Abfrage mit dem \texttt{langchain}-Framework gezeigt.\vspace{0.2cm}

\lstinputlisting[
	language=python,
	caption={Abfragen der Ollama Modelle mit \texttt{langchain}},
	label=lst:call_ollama_model_langchain
]{codes/src/execute/execute_ollama_langchain.py}

\newpage

Für die Optimierung mit dem \texttt{DSPy}-Frameowrk wurde der Code, welcher in Listing \ref{lst:call_ollama_model_dspy} gezeigt wird angewandt.

\lstinputlisting[
	language=python,
	caption={Abfragen der Ollama Modelle mit \texttt{DSPy}},
	label=lst:call_ollama_model_dspy
]{codes/src/execute/execute_ollama_dspy.py}


\newpage
\section{Abfrage cloudbasierter Modelle}
Das Listig \ref{lst:execution_gemini_models} zeigt die Umsetzung der Abfrage von den Gemini Modellen mit Python. Um die Abfragen durchzuführen sind neben dem Projektnamen und des Standortes, noch ein gültiger GCP Account erforderlich.

\lstinputlisting[
	language=python,
	caption={Ausführen der Prompts für Gemini Modelle.},
	label=lst:execution_gemini_models
]{codes/src/execute/execute_google_gemini.py}

\newpage

Ähnlich wie die Gemini Modelle, werden auch die ChatGPT Modelle von OpenAI abgefragt. Auch hier ist ein gültiger Account und ein API Key notwendig. Das Listing \ref{lst:execution_openai_models} zeigt die Abfrage mittels Python.

\lstinputlisting[
	language=python,
	caption={Ausführen der Prompts für OpenAI Modelle.},
	label=lst:execution_openai_models
]{codes/src/execute/execute_openai_gpt.py}

\newpage
\section{Evaluation der Antworten von den Modellen}
Das Listing \ref{lst:evaluation_search_generated_code} zeigt beispielhaft eine Suche nach Codezeilen in einer Antwort eines Modells. Die Methode \texttt{search\_generated\_code} muss auf jedes Modell angepasst sein.

\lstinputlisting[
language=python,
caption={Evaluation der Modellantworten: Suche nach Codeausschnitten},
label=lst:evaluation_search_generated_code
]{codes/src/evaluation/evaluation.py}


\newpage
\section{Methodenerkennung unter PHP}
Mit den folgenden Skripten wird gezeigt das es möglich ist PHP Funktionen welche durch LLMs erzeugt wurden zu scannen und beispielsweise deren Namen in Tests verwenden zu können. Das Listing \ref{lst:php_code_functions} zeigt ein Skript, das beispielhaft verschiedene Funktionen implementiert die durch das Skript im Listing \ref{lst:php_code_scan_functions} zur Laufzeit erkannt werden.\vspace{0.2cm}

\begin{lstlisting}[
	language=php,
	caption={PHP Skript welches verschiedene Funktionen imlementiert},
	label=lst:php_code_functions
]
<?php
// @file: methods.php

function fibfib_rekursiv($n) {}

function fibfib_iterativ($n) {}

function fib($x, $y) {}

function fibfib($n) {}

\end{lstlisting}

\begin{lstlisting}[
	language=php,
	caption={PHP Skript das Methoden zur Laufzeit erkennt},
	label=lst:php_code_scan_functions
]
<?php
// @file scan_methods.php

<?php

// include "method_names.php";
$filename = "method_names.php";
$code = file_get_contents($filename);
$tokens = token_get_all($code);

$all = [];
$search = "";
$func_name = False;
foreach ($tokens as $token) {
	if (
		gettype($token) == "array" &&
		count($token) > 1 &&
		trim($token[1]) != "" &&
		trim($token[1]) !="<?php"
	) {
		if ($token[1] == "function") {
			if (str_ends_with($search, ",")) {
				$search = substr($search, 0, strlen($search) - 1);
			}
			$search = $search . ")";
			if ($search  != ")") {
				$all[] = $search;
			}
			$search = $token[1] . " ";
			$func_name = True;
		} elseif ($func_name) {
			$func_name = False;
			$search = $search . $token[1] . "(";
		} else {
			$search = $search . $token[1] . ",";
		}
	}
}

if (str_ends_with($search, ",")) {
	$search = substr($search, 0, strlen($search) - 1);
}

$search = $search . ")";
$all[] = $search;

print_r($all);
\end{lstlisting}

Nach dem Ausführen des Skriptes wird die Ausgabe, welche in Listing \ref{bash_scan_function_result} zu sehen ist erzeugt.

\begin{lstlisting}[
	language=bash,
	caption={Erkannte PHP Methoden},
	label=lst:bash_scan_function_result
]
Array
(
    [0] => function fibfib_rekursiv($n)
    [1] => function fibfib_iterativ($n)
    [2] => function fib($x,$y)
    [3] => function fibfib($n)
)
\end{lstlisting}

