@online{noauthor_developers_2024,
	title = {Developers want more, more, more: the 2024 results from Stack Overflow’s Annual Developer Survey},
	author = {Yepis, Erin},
	url = {https://stackoverflow.blog/2024/07/24/developers-want-more-more-more-the-2024-results-from-stack-overflow-s-annual-developer-survey/},
	shorttitle = {Developers want more, more, more},
	urldate = {2024-08-09},
	date = {2024-07-24},
	langid = {english},
}

@report{definition_ki2019,
	author = {Ala-Pietilä, Pekka and Bauer, Wilhelm and Bergmann, Urs and Bielikova, Maria},
	date = {2019-03-05},
	title = {Eine Definition der KI: Wichtigste Fähigkeiten und Wissenschaftsgebiete},
	url = {https://elektro.at/wp-content/uploads/2019/10/EU_Definition-KI.pdf},
	urldate = {2024-09-10},
}

@article{yuen_universal_2021,
	title = {Universal activation function for machine learning},
	volume = {11},
	issn = {2045-2322},
	url = {https://doi.org/10.1038/s41598-021-96723-8},
	doi = {10.1038/s41598-021-96723-8},
	abstract = {This article proposes a universal activation function ({UAF}) that achieves near optimal performance in quantification, classification, and reinforcement learning ({RL}) problems. For any given problem, the gradient descent algorithms are able to evolve the {UAF} to a suitable activation function by tuning the {UAF}’s parameters. For the {CIFAR}-10 classification using the {VGG}-8 neural network, the {UAF} converges to the Mish like activation function, which has near optimal performance \$\$F\_\{1\}=0.902{\textbackslash}pm 0.004\$\$when compared to other activation functions. In the graph convolutional neural network on the {CORA} dataset, the {UAF} evolves to the identity function and obtains \$\$F\_1=0.835{\textbackslash}pm 0.008\$\$. For the quantification of simulated 9-gas mixtures in 30 {dB} signal-to-noise ratio ({SNR}) environments, the {UAF} converges to the identity function, which has near optimal root mean square error of \$\$0.489{\textbackslash}pm 0.003{\textasciitilde}{\textbackslash}mu \{{\textbackslash}mathrm\{M\}\}\$\$. In the {ZINC} molecular solubility quantification using graph neural networks, the {UAF} morphs to a {LeakyReLU}/Sigmoid hybrid and achieves {RMSE}=\$\$0.47{\textbackslash}pm 0.04\$\$. For the {BipedalWalker}-v2 {RL} dataset, the {UAF} achieves the 250 reward in \$\$\{961{\textbackslash}pm 193\}\$\$epochs with a brand new activation function, which gives the fastest convergence rate among the activation functions.},
	pages = {18757},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Scientific Reports},
	author = {Yuen, Brosnan and Hoang, Minh Tu and Dong, Xiaodai and Lu, Tao},
	date = {2021-09-21},
}

@online{brownlee-2021,
	author = {given-i=JB, given=Jason, family=Brownlee},
	date = {2021-01-22},
	title = {How to Choose an Activation Function for Deep Learning},
	url = {https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/},
	urldate = {2024-09-19},
}

@report{sharma-2020,
	author = {given=Siddharth, family=Sharma and given=Simone, family=Sharma and given=Anidhya, family=Athaiya and {Dept. of Computer Science and Engineering, Global Institute of Technology, Jaipur}},
	date = {2020-04},
	number = {Issue 12},
	pages = {310--316},
	title = {ACTIVATION FUNCTIONS IN NEURAL NETWORKS},
	url = {https://www.ijeast.com/papers/310-316,Tesma412,IJEAST.pdf},
	urldate = {2024-09-19},
	volume = {Vol. 4},
}

@online{cs231n_2024,
	title = {CS231n Convolutional Neural Networks for Visual Recognition},
	url = {https://cs231n.github.io/neural-networks-1/},
	urldate = {2024-09-20},
}

@article{rallabandi-2023,
	author = {given-i=S, given=Srikari, family=Rallabandi},
	date = {2023-03-27},
	title = {Activation functions: ReLU vs. Leaky ReLU - Srikari Rallabandi - Medium},
	url = {https://medium.com/@sreeku.ralla/activation-functions-relu-vs-leaky-relu-b8272dc0b1be},
}

@online{bhargav-2023,
	author = {given-i=NB, given=Nikhil, family=Bhargav},
	date = {2023-11-27},
	title = {ReLU vs. LeakyReLU vs. PReLU | Baeldung on Computer Science},
	url = {https://www.baeldung.com/cs/relu-vs-leakyrelu-vs-prelu},
	urldate = {2024-09-20},
}

@misc{goodfellow_maxout_2013,
	title = {Maxout Networks},
	url = {http://arxiv.org/abs/1302.4389},
	doi = {10.48550/arXiv.1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: {MNIST}, {CIFAR}-10, {CIFAR}-100, and {SVHN}.},
	number = {{arXiv}:1302.4389},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	urldate = {2024-09-20},
	date = {2013-09-20},
	eprinttype = {arxiv},
	eprint = {1302.4389 [cs, stat]},
	note = {version: 4},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\w.pahl\\Zotero\\storage\\BLIVUG35\\Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\w.pahl\\Zotero\\storage\\IXQ9W9WD\\1302.html:text/html},
}

@online{duden_unknown-author,
	title = {Der Umfang des deutschen Wortschatzes},
	date = {2020},
	url = {https://www.duden.de/sprachwissen/sprachratgeber/Zum-Umfang-des-deutschen-Wortschatzes},
	urldate = {2024-09-23},
}

@online{vaswani-2017,
	author = {given-i=A, given=Ashish, family=Vaswani and given-i=N, given=Noam, family=Shazeer and given-i=N, given=Niki, family=Parmar and given-i=J, given=Jakob, family=Uszkoreit and given-i=L, given=Llion, family=Jones and given-i=AN, given={Aidan N.}, family=Gomez and given-i=L, given=Lukasz, family=Kaiser and given-i=I, given=Illia, family=Polosukhin},
	date = {2017-06-12},
	title = {Attention is all you need},
	url = {https://arxiv.org/abs/1706.03762},
	urldate = {2024-09-23},
}

@article{goldberg-2016,
	author = {given-i=Y, given=Yoav, family=Goldberg},
	date = {2016-11-20},
	doi = {10.1613/jair.4992},
	journaltitle = {Journal of Artificial Intelligence Research},
	pages = {345--420},
	title = {A Primer on Neural Network Models for Natural Language Processing},
	url = {https://jair.org/index.php/jair/article/view/11030},
	volume = {57},
}

@image{pahl-2024,
	author = {given=Johanna, family=Pahl},
	date = {2024-09-26},
	title = {Zeichnung einer biologische Zelle},
}
