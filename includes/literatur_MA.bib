@online{noauthor_developers_2024,
	title = {Developers want more, more, more: the 2024 results from Stack Overflow’s Annual Developer Survey},
	author = {Yepis, Erin},
	url = {https://stackoverflow.blog/2024/07/24/developers-want-more-more-more-the-2024-results-from-stack-overflow-s-annual-developer-survey/},
	shorttitle = {Developers want more, more, more},
	urldate = {2024-08-09},
	date = {2024-07-24},
	langid = {english},
}

@report{definition_ki2019,
	author = {Ala-Pietilä, Pekka and Bauer, Wilhelm and Bergmann, Urs and Bielikova, Maria},
	date = {2019-03-05},
	title = {Eine Definition der KI: Wichtigste Fähigkeiten und Wissenschaftsgebiete},
	url = {https://elektro.at/wp-content/uploads/2019/10/EU_Definition-KI.pdf},
	urldate = {2024-09-10},
}

@article{yuen_universal_2021,
	title = {Universal activation function for machine learning},
	volume = {11},
	issn = {2045-2322},
	url = {https://doi.org/10.1038/s41598-021-96723-8},
	doi = {10.1038/s41598-021-96723-8},
	abstract = {This article proposes a universal activation function ({UAF}) that achieves near optimal performance in quantification, classification, and reinforcement learning ({RL}) problems. For any given problem, the gradient descent algorithms are able to evolve the {UAF} to a suitable activation function by tuning the {UAF}’s parameters. For the {CIFAR}-10 classification using the {VGG}-8 neural network, the {UAF} converges to the Mish like activation function, which has near optimal performance \$\$F\_\{1\}=0.902{\textbackslash}pm 0.004\$\$when compared to other activation functions. In the graph convolutional neural network on the {CORA} dataset, the {UAF} evolves to the identity function and obtains \$\$F\_1=0.835{\textbackslash}pm 0.008\$\$. For the quantification of simulated 9-gas mixtures in 30 {dB} signal-to-noise ratio ({SNR}) environments, the {UAF} converges to the identity function, which has near optimal root mean square error of \$\$0.489{\textbackslash}pm 0.003{\textasciitilde}{\textbackslash}mu \{{\textbackslash}mathrm\{M\}\}\$\$. In the {ZINC} molecular solubility quantification using graph neural networks, the {UAF} morphs to a {LeakyReLU}/Sigmoid hybrid and achieves {RMSE}=\$\$0.47{\textbackslash}pm 0.04\$\$. For the {BipedalWalker}-v2 {RL} dataset, the {UAF} achieves the 250 reward in \$\$\{961{\textbackslash}pm 193\}\$\$epochs with a brand new activation function, which gives the fastest convergence rate among the activation functions.},
	pages = {18757},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Scientific Reports},
	author = {Yuen, Brosnan and Hoang, Minh Tu and Dong, Xiaodai and Lu, Tao},
	date = {2021-09-21},
}

@online{brownlee-2021,
	author = {given-i=JB, given=Jason, family=Brownlee},
	date = {2021-01-22},
	title = {How to Choose an Activation Function for Deep Learning},
	url = {https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/},
	urldate = {2024-09-19},
}

@report{sharma-2020,
	author = {given=Siddharth, family=Sharma and given=Simone, family=Sharma and given=Anidhya, family=Athaiya and {Dept. of Computer Science and Engineering, Global Institute of Technology, Jaipur}},
	date = {2020-04},
	number = {Issue 12},
	pages = {310--316},
	title = {{A}ctivation {F}unctions in {N}eural {N}etworks},
	url = {https://www.ijeast.com/papers/310-316,Tesma412,IJEAST.pdf},
	urldate = {2024-09-19},
	volume = {Vol. 4},
}

@online{cs231n_2024,
	title = {CS231n Convolutional Neural Networks for Visual Recognition},
	url = {https://cs231n.github.io/neural-networks-1/},
	urldate = {2024-09-20},
}

@article{rallabandi-2023,
	author = {given-i=S, given=Srikari, family=Rallabandi},
	date = {2023-03-27},
	title = {Activation functions: ReLU vs. Leaky ReLU - Srikari Rallabandi - Medium},
	url = {https://medium.com/@sreeku.ralla/activation-functions-relu-vs-leaky-relu-b8272dc0b1be},
}

@online{bhargav-2023,
	author = {given-i=NB, given=Nikhil, family=Bhargav},
	date = {2023-11-27},
	title = {ReLU vs. LeakyReLU vs. PReLU | Baeldung on Computer Science},
	url = {https://www.baeldung.com/cs/relu-vs-leakyrelu-vs-prelu},
	urldate = {2024-09-20},
}

@misc{goodfellow_maxout_2013,
	title = {Maxout Networks},
	url = {http://arxiv.org/abs/1302.4389},
	doi = {10.48550/arXiv.1302.4389},
	abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: {MNIST}, {CIFAR}-10, {CIFAR}-100, and {SVHN}.},
	number = {{arXiv}:1302.4389},
	publisher = {{arXiv}},
	author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
	urldate = {2024-09-20},
	date = {2013-09-20},
	eprinttype = {arxiv},
	eprint = {1302.4389 [cs, stat]},
	note = {version: 4},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\w.pahl\\Zotero\\storage\\BLIVUG35\\Goodfellow et al. - 2013 - Maxout Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\w.pahl\\Zotero\\storage\\IXQ9W9WD\\1302.html:text/html},
}

@online{duden_unknown-author,
	title = {Der Umfang des deutschen Wortschatzes},
	date = {2020},
	url = {https://www.duden.de/sprachwissen/sprachratgeber/Zum-Umfang-des-deutschen-Wortschatzes},
	urldate = {2024-09-23},
}

@online{vaswani-2017,
	author = {given-i=A, given=Ashish, family=Vaswani and given-i=N, given=Noam, family=Shazeer and given-i=N, given=Niki, family=Parmar and given-i=J, given=Jakob, family=Uszkoreit and given-i=L, given=Llion, family=Jones and given-i=AN, given={Aidan N.}, family=Gomez and given-i=L, given=Lukasz, family=Kaiser and given-i=I, given=Illia, family=Polosukhin},
	date = {2017-06-12},
	title = {Attention is all you need},
	url = {https://arxiv.org/abs/1706.03762},
	urldate = {2024-09-23},
}

@article{goldberg-2016,
	author = {given-i=Y, given=Yoav, family=Goldberg},
	date = {2016-11-20},
	doi = {10.1613/jair.4992},
	journaltitle = {Journal of Artificial Intelligence Research},
	pages = {345--420},
	title = {A Primer on Neural Network Models for Natural Language Processing},
	url = {https://jair.org/index.php/jair/article/view/11030},
	volume = {57},
}

@image{pahl-2024,
	author = {given=Johanna, family=Pahl},
	date = {2024-09-26},
	title = {Zeichnung einer biologische Zelle},
}

@inproceedings{handschuh-2024,
	author = {given-i=SH, given=Siegfried, family=Handschuh},
	date = {2024-05-06},
	title = {Grosse Sprachmodelle},
	url = {https://bop.unibe.ch/iw/article/view/11053/13941},
	urldate = {2024-09-28},
	eventtitle = {Travaux du/Arbeiten aus dem Master of Advanced Studies in Archival Band 8 Nr. 1},
	publisher = {Gesellschaft für Informatik e.V.},
	langid = {german},
}

@online{du-2024,
	author = {given-i=Z, given=Zhuoyun, family=Du and given-i=C, given=Chen, family=Qian and given-i=W, given=Wei, family=Liu and given-i=Z, given=Zihao, family=Xie and given-i=Y, given=Yifei, family=Wang and given-i=Y, given=Yufan, family=Dang and given-i=W, given=Weize, family=Chen and given-i=C, given=Cheng, family=Yang},
	date = {2024-06-13},
	title = {Multi-Agent Software Development through Cross-Team Collaboration},
	url = {https://arxiv.org/abs/2406.08979},
	urldate = {2024-10-04},
}

% -- Prompt engineering
@online{zhou-2022,
	author = {given-i=Y, given=Yongchao, family=Zhou and given-i=AI, given={Andrei Ioan}, family=Muresanu and given-i=Z, given=Ziwen, family=Han and given-i=K, given=Keiran, family=Paster and given-i=S, given=Silviu, family=Pitis and given-i=H, given=Harris, family=Chan and given-i=J, given=Jimmy, family=Ba},
	date = {2022-11-03},
	title = {Large language models are Human-Level prompt engineers},
	url = {https://arxiv.org/abs/2211.01910},
	urldate = {2024-10-12},
}

@online{amatriain-2024,
	author = {given-i=X, given=Xavier, family=Amatriain},
	date = {2024-01-24},
	title = {Prompt Design and Engineering: Introduction and Advanced Methods},
	url = {https://arxiv.org/abs/2401.14423v3},
	urldate = {2024-10-12},
}

@online{wei-2021,
	author = {given-i=J, given=Jason, family=Wei and given-i=M, given=Maarten, family=Bosma and given-i=VY, given={Vincent Y.}, family=Zhao and given-i=K, given=Kelvin, family=Guu and given-i=AW, given={Adams Wei}, family=Yu and given-i=B, given=Brian, family=Lester and given-i=N, given=Nan, family=Du and given-i=AM, given={Andrew M.}, family=Dai and given-i=Q, given=Quoc, family=Le, suffix=V},
	date = {2021-09-03},
	title = {Finetuned language models are Zero-Shot learners},
	url = {https://arxiv.org/abs/2109.01652},
	urldate = {2024-10-12},
}

@online{brown-2020,
	author = {given-i=TB, given={Tom B.}, family=Brown and given-i=B, given=Benjamin, family=Mann and given-i=N, given=Nick, family=Ryder and given-i=M, given=Melanie, family=Subbiah and given-i=J, given=Jared, family=Kaplan and given-i=P, given=Prafulla, family=Dhariwal and given-i=A, given=Arvind, family=Neelakantan and given-i=P, given=Pranav, family=Shyam and given-i=G, given=Girish, family=Sastry and given-i=A, given=Amanda, family=Askell and given-i=S, given=Sandhini, family=Agarwal and given-i=A, given=Ariel, family=Herbert-Voss and given-i=G, given=Gretchen, family=Krueger and given-i=T, given=Tom, family=Henighan and given-i=R, given=Rewon, family=Child and given-i=A, given=Aditya, family=Ramesh and given-i=DM, given={Daniel M.}, family=Ziegler and given-i=J, given=Jeffrey, family=Wu and given-i=C, given=Clemens, family=Winter and given-i=C, given=Christopher, family=Hesse and given-i=M, given=Mark, family=Chen and given-i=E, given=Eric, family=Sigler and given-i=M, given=Mateusz, family=Litwin and given-i=S, given=Scott, family=Gray and given-i=B, given=Benjamin, family=Chess and given-i=J, given=Jack, family=Clark and given-i=C, given=Christopher, family=Berner and given-i=S, given=Sam, family=McCandlish and given-i=A, given=Alec, family=Radford and given-i=I, given=Ilya, family=Sutskever and given-i=D, given=Dario, family=Amodei},
	date = {2020-05-28},
	title = {Language Models are Few-Shot Learners},
	url = {https://arxiv.org/abs/2005.14165},
	urldate = {2024-10-12},
}

@online{min-2022,
	author = {given-i=S, given=Sewon, family=Min and given-i=X, given=Xinxi, family=Lyu and given-i=A, given=Ari, family=Holtzman and given-i=M, given=Mikel, family=Artetxe and given-i=M, given=Mike, family=Lewis and given-i=H, given=Hannaneh, family=Hajishirzi and given-i=L, given=Luke, family=Zettlemoyer},
	date = {2022-02-25},
	title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
	url = {https://arxiv.org/abs/2202.12837},
	urldate = {2024-10-12},
}

@online{wei-2022,
	author = {given-i=J, given=Jason, family=Wei and given-i=X, given=Xuezhi, family=Wang and given-i=D, given=Dale, family=Schuurmans and given-i=M, given=Maarten, family=Bosma and given-i=B, given=Brian, family=Ichter and given-i=F, given=Fei, family=Xia and given-i=E, given=Ed, family=Chi and given-i=Q, given=Quoc, family=Le and given-i=D, given=Denny, family=Zhou},
	date = {2022-01-28},
	title = {Chain-of-Thought prompting elicits reasoning in large language models},
	url = {https://arxiv.org/abs/2201.11903},
	urldate = {2024-10-12},
}

@online{zhang-2023,
	author = {given-i=Y, given=Yifan, family=Zhang and given-i=Y, given=Yang, family=Yuan and given-i=AC, given={Andrew Chi-Chih}, family=Yao},
	date = {2023-11-20},
	title = {Meta Prompting for AI Systems},
	url = {https://arxiv.org/abs/2311.11482},
	urldate = {2024-10-12},
}

@online{long-2023,
	author = {given-i=J, given=Jieyi, family=Long},
	date = {2023-05-15},
	title = {Large language model guided Tree-of-Thought},
	url = {https://arxiv.org/abs/2305.08291},
	urldate = {2024-10-14},
}

@online{yao-2023,
	author = {given-i=S, given=Shunyu, family=Yao and given-i=D, given=Dian, family=Yu and given-i=J, given=Jeffrey, family=Zhao and given-i=I, given=Izhak, family=Shafran and given-i=TL, given={Thomas L.}, family=Griffiths and given-i=Y, given=Yuan, family=Cao and given-i=K, given=Karthik, family=Narasimhan},
	date = {2023-05-17},
	title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
	url = {https://arxiv.org/abs/2305.10601},
	urldate = {2024-10-14},
}

% ----------

@book{banholzer-2020,
	author = {given={Volker M.}, family=Banholzer},
	date = {2020},
	publisher = {Technische Hochschule Nürnberg Georg-Simon-Ohm},
	title = {Künstliche Intelligenz als Treiber der Veränderung in der Unternehmenskommunikation 4.0?},
	url = {https://www.th-nuernberg.de/fileadmin/fakultaeten/amp/amp_docs/K%C3%BCnstliche_Intelligenz_und_die_Rolle_n__von_Unternehmenskommunikation_Banholzer_IKOM_WP_1_2020__fin-1.pdf},
	volume = {1/2020},
}

@report{oswald-2022,
	date = {2022},
	edition = {2. Auflage},
	editor = {given=Gerhard, family=Oswald and given=Thomas, family=Saueressig and {Helmut Krcmar}},
	publisher = {Springer},
	title = {Digitale Transformation: Fallbeispiele und Branchenanalysen},
	url = {https://library.oapen.org/bitstream/handle/20.500.12657/57358/978-3-658-37571-3.pdf?sequence=1&utm_source=textcortex&utm_medium=zenochat#page=70},
	urldate = {2024-10-19},
}

@inproceedings{hartenstein_2024,
	title = {{KI}-gestützte Modernisierung von Altanwendungen: Anwendungsfelder von {LLMs} im Software Reengineering},
	url = {https://dl.gi.de/handle/20.500.12116/44181},
	shorttitle = {{KI}-gestützte Modernisierung von Altanwendungen},
	abstract = {Die Integration von Large Language Models, kurz {LLMs}, in den Modernisierungsprozess von Altanwendungen bietet nicht nur eine Vielzahl technologischer Möglichkeiten, sondern dient auch als starke Motivation für Unternehmen, ihre bestehenden Systeme zu verbessern. {LLMs} repräsentieren einen bedeutsamen Fortschritt in der künstlichen Intelligenz ({KI}), veraltete Anwendungen können mit, aber auch durch {LLMs} aufgewertet werden. Diese Ausarbeitung adressiert die folgenden Fragen zur Implementierung von {LLMs} im Modernisierungsprozess: {FF}1 Wie können {LLMs} die Modernisierung von Altan wendungen im Anforderungsmanagement unter stützen? {FF}2 Inwiefern ermöglichen {LLMs} effiziente Software Reengineering Prozesse?},
	eventtitle = {Softwaretechnik-Trends Band 44, Heft 2},
	publisher = {Gesellschaft für Informatik e.V.},
	author = {Hartenstein, Sandro and Schmietendorf, Andreas},
	urldate = {2024-08-15},
	date = {2024},
	langid = {german},
}
